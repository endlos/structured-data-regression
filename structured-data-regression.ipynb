{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Development and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will cover the core parts of the machine learning workflow, running locally within the Google Cloud Datalab environment. Local development and validation, along with using a sample of the full dataset, is recommended as a starting point. This allows for a shorter development-validation iteration cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workspace Setup\n",
    "\n",
    "The first step is to setup the workspace that we will use within this notebook - the python libraries, and the local directory containing the data inputs and outputs produced over the course of the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import google.datalab.ml as ml\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plot\n",
    "import mltoolbox.regression.dnn as regression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local development workspace will be in `/content/datalab/workspace/census` by default.\n",
    "\n",
    "Note that the `/content/datalab` directory is physically located within the data disk mounted into the Datalab instance, but outside of the git repository containing notebooks, which makes it suitable for storing data files and generated files that are useful to keep around while you are working on a project, but do not belong in the source repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_path = '/content/datalab/workspace/census'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {workspace_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: If you have previously run this notebook, and want to start from scratch, then run the next cell to delete and create the workspace directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {workspace_path} && mkdir {workspace_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we will copy the data into this workspace. Generally, in your own work, you will need to create a representative sample dataset to use for local development, while leaving the full dataset to use when running on the service. For purposes of the sample, which uses a relatively small dataset, we'll copy it down in entirety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "Updates are available for some Cloud SDK components.  To install them,\r\n",
      "please run:\r\n",
      "  $ gcloud components update\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil -q cp gs://cloud-datalab-samples/census/ss14psd.csv {workspace_path}/data/census.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8000\r\n",
      "-rw-r--r-- 1 root root 8189323 Oct  1 23:20 census.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l {workspace_path}/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "Its a good idea to load data and inspect it to build an understanding of the structure, as well as preparation steps that will be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8626 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RT</th>\n",
       "      <th>SERIALNO</th>\n",
       "      <th>SPORDER</th>\n",
       "      <th>PUMA</th>\n",
       "      <th>ST</th>\n",
       "      <th>ADJINC</th>\n",
       "      <th>PWGTP</th>\n",
       "      <th>AGEP</th>\n",
       "      <th>CIT</th>\n",
       "      <th>CITWP</th>\n",
       "      <th>...</th>\n",
       "      <th>pwgtp71</th>\n",
       "      <th>pwgtp72</th>\n",
       "      <th>pwgtp73</th>\n",
       "      <th>pwgtp74</th>\n",
       "      <th>pwgtp75</th>\n",
       "      <th>pwgtp76</th>\n",
       "      <th>pwgtp77</th>\n",
       "      <th>pwgtp78</th>\n",
       "      <th>pwgtp79</th>\n",
       "      <th>pwgtp80</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P</td>\n",
       "      <td>25</td>\n",
       "      <td>01</td>\n",
       "      <td>00400</td>\n",
       "      <td>46</td>\n",
       "      <td>1008425</td>\n",
       "      <td>00038</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>00011</td>\n",
       "      <td>00010</td>\n",
       "      <td>00008</td>\n",
       "      <td>00032</td>\n",
       "      <td>00040</td>\n",
       "      <td>00038</td>\n",
       "      <td>00033</td>\n",
       "      <td>00040</td>\n",
       "      <td>00068</td>\n",
       "      <td>00090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P</td>\n",
       "      <td>467</td>\n",
       "      <td>01</td>\n",
       "      <td>00600</td>\n",
       "      <td>46</td>\n",
       "      <td>1008425</td>\n",
       "      <td>00120</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>00119</td>\n",
       "      <td>00039</td>\n",
       "      <td>00034</td>\n",
       "      <td>00119</td>\n",
       "      <td>00125</td>\n",
       "      <td>00104</td>\n",
       "      <td>00119</td>\n",
       "      <td>00102</td>\n",
       "      <td>00184</td>\n",
       "      <td>00038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P</td>\n",
       "      <td>490</td>\n",
       "      <td>01</td>\n",
       "      <td>00500</td>\n",
       "      <td>46</td>\n",
       "      <td>1008425</td>\n",
       "      <td>00172</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>00049</td>\n",
       "      <td>00045</td>\n",
       "      <td>00148</td>\n",
       "      <td>00163</td>\n",
       "      <td>00313</td>\n",
       "      <td>00174</td>\n",
       "      <td>00153</td>\n",
       "      <td>00165</td>\n",
       "      <td>00158</td>\n",
       "      <td>00126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P</td>\n",
       "      <td>490</td>\n",
       "      <td>02</td>\n",
       "      <td>00500</td>\n",
       "      <td>46</td>\n",
       "      <td>1008425</td>\n",
       "      <td>00113</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>00037</td>\n",
       "      <td>00032</td>\n",
       "      <td>00107</td>\n",
       "      <td>00125</td>\n",
       "      <td>00249</td>\n",
       "      <td>00107</td>\n",
       "      <td>00105</td>\n",
       "      <td>00094</td>\n",
       "      <td>00091</td>\n",
       "      <td>00079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P</td>\n",
       "      <td>676</td>\n",
       "      <td>01</td>\n",
       "      <td>00300</td>\n",
       "      <td>46</td>\n",
       "      <td>1008425</td>\n",
       "      <td>00023</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>00022</td>\n",
       "      <td>00006</td>\n",
       "      <td>00035</td>\n",
       "      <td>00030</td>\n",
       "      <td>00028</td>\n",
       "      <td>00027</td>\n",
       "      <td>00020</td>\n",
       "      <td>00042</td>\n",
       "      <td>00019</td>\n",
       "      <td>00011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 284 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  RT SERIALNO SPORDER   PUMA  ST   ADJINC  PWGTP AGEP CIT CITWP   ...    \\\n",
       "0  P       25      01  00400  46  1008425  00038   85   1         ...     \n",
       "1  P      467      01  00600  46  1008425  00120   62   1         ...     \n",
       "2  P      490      01  00500  46  1008425  00172   64   1         ...     \n",
       "3  P      490      02  00500  46  1008425  00113   67   1         ...     \n",
       "4  P      676      01  00300  46  1008425  00023   89   1         ...     \n",
       "\n",
       "  pwgtp71 pwgtp72 pwgtp73 pwgtp74 pwgtp75 pwgtp76 pwgtp77 pwgtp78 pwgtp79  \\\n",
       "0   00011   00010   00008   00032   00040   00038   00033   00040   00068   \n",
       "1   00119   00039   00034   00119   00125   00104   00119   00102   00184   \n",
       "2   00049   00045   00148   00163   00313   00174   00153   00165   00158   \n",
       "3   00037   00032   00107   00125   00249   00107   00105   00094   00091   \n",
       "4   00022   00006   00035   00030   00028   00027   00020   00042   00019   \n",
       "\n",
       "  pwgtp80  \n",
       "0   00090  \n",
       "1   00038  \n",
       "2   00126  \n",
       "3   00079  \n",
       "4   00011  \n",
       "\n",
       "[5 rows x 284 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv(os.path.join(workspace_path, 'data/census.csv'), dtype=str)\n",
    "print '%d rows' % len(df_data)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The census data contains a large number of columns. Only a few are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Transformations\n",
    "\n",
    "The raw census data requires a number of transformations before it is usable for machine learning:\n",
    "\n",
    "1. Apply understanding of the domain and the problem to determine which data to include or join, as well as which data to filter out if it is adding noise. In the case of census, we'll pick just a few of the many columns present in the dataset.\n",
    "2. Handle missing values, or variations in formatting.\n",
    "3. Apply other transformations in support of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is packaged as a function that can be reused if you need to apply to future\n",
    "# datasets, esp. to prediction data, to ensure consistent transformations are applied.\n",
    "\n",
    "def transform_data(df):\n",
    "  interesting_columns = ['WAGP','SERIALNO','AGEP','COW','ESP','ESR','FOD1P','HINS4','INDP',\n",
    "                         'JWMNP', 'JWTR', 'MAR', 'POWPUMA', 'PUMA', 'RAC1P', 'SCHL',\n",
    "                         'SCIENGRLP', 'SEX', 'WKW']\n",
    "  df = df[interesting_columns]\n",
    "  \n",
    "  # Replace whitespace with NaN, and NaNs with empty string\n",
    "  df = df.replace('\\s+', np.nan, regex=True).fillna('')\n",
    "\n",
    "  # Filter out the rows without an income, i.e. there is no target value to learn from\n",
    "  df = df[df.WAGP != '']\n",
    "  \n",
    "  # Convert the wage value into units of 1000. So someone making an income from wages\n",
    "  # of $23200 will have it encoded as 23.2\n",
    "  df['WAGP'] = df.WAGP.astype(np.int64) / 1000.0\n",
    "\n",
    "  # Filter out rows with income values we don't care about, i.e. outliers\n",
    "  # Filter out rows with less than 10K and more than 150K\n",
    "  df = df[(df.WAGP >= 10.0) & (df.WAGP < 150.0)]\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3390 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WAGP</th>\n",
       "      <th>SERIALNO</th>\n",
       "      <th>AGEP</th>\n",
       "      <th>COW</th>\n",
       "      <th>ESP</th>\n",
       "      <th>ESR</th>\n",
       "      <th>FOD1P</th>\n",
       "      <th>HINS4</th>\n",
       "      <th>INDP</th>\n",
       "      <th>JWMNP</th>\n",
       "      <th>JWTR</th>\n",
       "      <th>MAR</th>\n",
       "      <th>POWPUMA</th>\n",
       "      <th>PUMA</th>\n",
       "      <th>RAC1P</th>\n",
       "      <th>SCHL</th>\n",
       "      <th>SCIENGRLP</th>\n",
       "      <th>SEX</th>\n",
       "      <th>WKW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70.0</td>\n",
       "      <td>467</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>2102</td>\n",
       "      <td>2</td>\n",
       "      <td>6990</td>\n",
       "      <td>015</td>\n",
       "      <td>01</td>\n",
       "      <td>3</td>\n",
       "      <td>00590</td>\n",
       "      <td>00600</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.0</td>\n",
       "      <td>490</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>8090</td>\n",
       "      <td>015</td>\n",
       "      <td>01</td>\n",
       "      <td>1</td>\n",
       "      <td>00590</td>\n",
       "      <td>00500</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>70.0</td>\n",
       "      <td>1225</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>5301</td>\n",
       "      <td>2</td>\n",
       "      <td>9680</td>\n",
       "      <td>015</td>\n",
       "      <td>01</td>\n",
       "      <td>1</td>\n",
       "      <td>00100</td>\n",
       "      <td>00100</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18.0</td>\n",
       "      <td>1225</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>8680</td>\n",
       "      <td>020</td>\n",
       "      <td>01</td>\n",
       "      <td>1</td>\n",
       "      <td>00100</td>\n",
       "      <td>00100</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>45.0</td>\n",
       "      <td>1438</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>6170</td>\n",
       "      <td>030</td>\n",
       "      <td>01</td>\n",
       "      <td>1</td>\n",
       "      <td>00100</td>\n",
       "      <td>00100</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    WAGP SERIALNO AGEP COW ESP ESR FOD1P HINS4  INDP JWMNP JWTR MAR POWPUMA  \\\n",
       "1   70.0      467   62   1       1  2102     2  6990   015   01   3   00590   \n",
       "2   19.0      490   64   2       1           2  8090   015   01   1   00590   \n",
       "6   70.0     1225   32   5       4  5301     2  9680   015   01   1   00100   \n",
       "7   18.0     1225   30   1       1           2  8680   020   01   1   00100   \n",
       "11  45.0     1438   55   1       1           2  6170   030   01   1   00100   \n",
       "\n",
       "     PUMA RAC1P SCHL SCIENGRLP SEX WKW  \n",
       "1   00600     1   21         2   1   1  \n",
       "2   00500     1   18             2   1  \n",
       "6   00100     1   21         2   1   1  \n",
       "7   00100     1   16             2   1  \n",
       "11  00100     1   16             1   1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = transform_data(df_data)\n",
    "print '%d rows' % len(df_data)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataSets\n",
    "\n",
    "Once the data is ready, the next step is to split data into training and evaluation datasets. In this sample, rows are split randomly in an 80/20 manner. Additionally, the schema is also saved for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_schema(df):\n",
    "  fields = []\n",
    "  for name, dtype in zip(df.columns, df.dtypes):\n",
    "    if dtype in (np.str, np.object):\n",
    "      # Categorical columns should have type 'STRING'\n",
    "      fields.append({'name': name, 'type': 'STRING'})\n",
    "    elif dtype in (np.int32, np.int64, np.float32, np.float64):\n",
    "      # Numerical columns have type 'FLOAT'\n",
    "      fields.append({'name': name, 'type': 'FLOAT'})\n",
    "    else:\n",
    "      raise ValueError('Unsupported column type \"%s\" in column \"%s\"' % (str(dtype), name))\n",
    "  return fields\n",
    "\n",
    "def create_datasets(df):\n",
    "  # Numbers in the range of [0, 1)\n",
    "  random_values = np.random.rand(len(df))\n",
    "\n",
    "  # Split data into %80, 20% partitions\n",
    "  df_train = df[random_values < 0.8]\n",
    "  df_eval = df[random_values >= 0.8]\n",
    "\n",
    "  return df_train, df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_eval = create_datasets(df_data)\n",
    "schema = create_schema(df_data)\n",
    "\n",
    "training_data_path = os.path.join(workspace_path, 'data/train.csv')\n",
    "eval_data_path = os.path.join(workspace_path, 'data/eval.csv')\n",
    "schema_path = os.path.join(workspace_path, 'data/schema.json')\n",
    "\n",
    "df_train.to_csv(training_data_path, header=False, index=False)\n",
    "df_eval.to_csv(eval_data_path, header=False, index=False)\n",
    "\n",
    "with open(schema_path, 'w') as f:\n",
    "  f.write(json.dumps(schema, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8204\r\n",
      "-rw-r--r-- 1 root root 8189323 Oct  1 23:20 census.csv\r\n",
      "-rw-r--r-- 1 root root   40352 Oct  1 23:21 eval.csv\r\n",
      "-rw-r--r-- 1 root root     998 Oct  1 23:21 schema.json\r\n",
      "-rw-r--r-- 1 root root  163203 Oct  1 23:21 train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l {workspace_path}/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create DataSet objects which are reference to one or more files identified by a path (or path pattern) along with associated schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ml.CsvDataSet(file_pattern=training_data_path, schema_file=schema_path)\n",
    "eval_data = ml.CsvDataSet(file_pattern=eval_data_path, schema_file=schema_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Data\n",
    "\n",
    "When building a model, a number of pieces of information about the training data are required - for example, the list of entries or vocabulary of a categorical/discrete column, or aggregate statistics like min and max for numerical columns. These require a full pass over the training data, and is usually done once, and needs to be repeated once if you change the schema in a future iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyze: completed\n"
     ]
    }
   ],
   "source": [
    "analysis_path = os.path.join(workspace_path, 'analysis')\n",
    "\n",
    "regression.analyze(dataset=train_data, output_dir=analysis_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of analysis is a stats file that contains analysis from the numerical columns, and a vocab file from each categorical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema.json\tvocab_ESR.csv\t vocab_JWTR.csv     vocab_SCHL.csv\r\n",
      "stats.json\tvocab_FOD1P.csv  vocab_MAR.csv\t    vocab_SCIENGRLP.csv\r\n",
      "vocab_AGEP.csv\tvocab_HINS4.csv  vocab_POWPUMA.csv  vocab_SERIALNO.csv\r\n",
      "vocab_COW.csv\tvocab_INDP.csv\t vocab_PUMA.csv     vocab_SEX.csv\r\n",
      "vocab_ESP.csv\tvocab_JWMNP.csv  vocab_RAC1P.csv    vocab_WKW.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls {analysis_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "All the data is in place to start training. A model learns to predict the target value (the income, 'WAGP'), based on the different pieces of input data (the various columns or features). The target and inputs are defined as features derived from the input data by applying a set of transforms to the columns.\n",
    "\n",
    "Additionally there is a special key column - this is any column in the data that can be used to uniquely identify instances. The value of this column is ignored during training, but this value is quite useful when using the resulting model during prediction as discussed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\n",
    "  \"WAGP\": {\"transform\": \"target\"},\n",
    "  \"SERIALNO\": {\"transform\": \"key\"},\n",
    "  \"AGEP\": {\"transform\": \"embedding\", \"embedding_dim\": 2},  # Age\n",
    "  \"COW\": {\"transform\": \"one_hot\"},                         # Class of worker\n",
    "  \"ESP\": {\"transform\": \"embedding\", \"embedding_dim\": 2},   # Employment status of parents\n",
    "  \"ESR\": {\"transform\": \"one_hot\"},                         # Employment status\n",
    "  \"FOD1P\": {\"transform\": \"embedding\", \"embedding_dim\": 3}, # Field of degree\n",
    "  \"HINS4\": {\"transform\": \"one_hot\"},                       # Medicaid\n",
    "  \"INDP\": {\"transform\": \"embedding\", \"embedding_dim\": 5},  # Industry\n",
    "  \"JWMNP\": {\"transform\": \"embedding\", \"embedding_dim\": 2}, # Travel time to work\n",
    "  \"JWTR\": {\"transform\": \"one_hot\"},                        # Transportation\n",
    "  \"MAR\": {\"transform\": \"one_hot\"},                         # Marital status\n",
    "  \"POWPUMA\": {\"transform\": \"one_hot\"},                     # Place of work\n",
    "  \"PUMA\": {\"transform\": \"one_hot\"},                        # Area code\n",
    "  \"RAC1P\": {\"transform\": \"one_hot\"},                       # Race\n",
    "  \"SCHL\": {\"transform\": \"one_hot\"},                        # School\n",
    "  \"SCIENGRLP\": {\"transform\": \"one_hot\"},                   # Science\n",
    "  \"SEX\": {\"transform\": \"one_hot\"},\n",
    "  \"WKW\": {\"transform\": \"one_hot\"}                          # Weeks worked\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = os.path.join(workspace_path, 'training')\n",
    "regression.train(train_dataset=train_data, eval_dataset=eval_data,\n",
    "                 output_dir=training_path,\n",
    "                 analysis_dir=analysis_path,\n",
    "                 features=features,\n",
    "                 max_steps=2000,\n",
    "                 layer_sizes=[5, 5, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard\n",
    "\n",
    "A training job produces various summary events containing values of metrics (eg. throughput and loss) over the course of its execution. These events can be observed in TensorBoard while the job executes or after it is executed.\n",
    "\n",
    "In this sample, training was short, and has completed. In the general case, especially for longer cloud training jobs, it is more interesting to launch TensorBoard while the training job continues to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_pid = ml.TensorBoard.start(training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.TensorBoard.stop(tensorboard_pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Trained Model\n",
    "\n",
    "It is interesting to get a sense of all the outputs produced during training, in addition to the summary event files, visualized in the previous step. In particular, note that the model is produced in a `model` subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -R {training_path}/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Once a model has been trained, it is necessary to evaluate it and understand how well it is performing. In order to evaluate a model, batch prediction jobs can be run against the one or more evaluation datasets that you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_path = os.path.join(workspace_path, 'evaluation')\n",
    "\n",
    "# Note the use of evaluation mode (as opposed to prediction mode). This is used to indicate the data being\n",
    "# predicted on contains a target value column (prediction data is missing that column).\n",
    "regression.batch_predict(training_dir=training_path,\n",
    "                         prediction_input_file=eval_data_path,\n",
    "                         output_dir=evaluation_path,\n",
    "                         output_format='json',\n",
    "                         mode='evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l {evaluation_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_json(os.path.join(evaluation_path, 'predictions-00000-of-00001.json'), lines=True)\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = metrics.mean_squared_error(df_eval['target'], df_eval['predicted'])\n",
    "rmse = math.sqrt(mse)\n",
    "print 'Root Mean Squared Error: %.3f' % rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval['error'] = df_eval['predicted'] - df_eval['target']\n",
    "_ = plot.hist(df_eval['error'], bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The root mean squared error and distribution of errors indicates how the model is performing at an aggregate level as well as indicative of the span of error values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Now that a model has been trained, and saved on-disk, it can be reloaded using TensorFlow, and be used to produce predictions, i.e. produce the income value given a set of new instances, or features that were not previously present in the training data. This mechanism can also help validate the model - it can be used to produce predictions for one or more evaluation datasets.\n",
    "\n",
    "Note that prediction data must be of the same type (input format, and order of columns) as the data that was used for training. The only difference is the first column, the target income value, is absent.\n",
    "\n",
    "Since the model is a regression model, a single value is the output of the prediction.\n",
    "\n",
    "Also note that second column in our schema was specified as a key column. This value of the key will accompany the output values, so they can be joined with the input instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%file {workspace_path}/data/prediction.csv\n",
    "SERIALNO,AGEP,COW,ESP,ESR,FOD1P,HINS4,INDP,JWMNP,JWTR,MAR,POWPUMA,PUMA,RAC1P,SCHL,SCIENGRLP,SEX,WKW\n",
    "490,64,2,0,1,0,2,8090,015,01,1,00590,00500,1,18,0,2,1\n",
    "1225,32,5,0,4,5301,2,9680,015,01,1,00100,00100,1,21,2,1,1\n",
    "1226,30,1,0,1,0,2,8680,020,01,1,00100,00100,1,16,0,2,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instances = pd.read_csv(os.path.join(workspace_path, 'data/prediction.csv'))\n",
    "df_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = regression.predict(training_dir=training_path, data=df_instances)\n",
    "df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index the instances DataFrame using the SERIALNO column, and join the predictions\n",
    "# DataFrame using the same column.\n",
    "df_instances.set_index(keys=['SERIALNO'], inplace=True)\n",
    "df_predictions.set_index(keys=['SERIALNO'], inplace=True)\n",
    "\n",
    "df_data = df_predictions.join(other=df_instances)\n",
    "df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "This notebook covered key stages of the workflow locally - data preparation, data analysis, training, and prediction. Once there is a working model, the next step is to use the full dataset, and scale to much larger data volumes by performing these steps in cloud using BigQuery, Machine Learning Engine, and Dataflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
